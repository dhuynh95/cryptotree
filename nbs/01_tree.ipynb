{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tree\n",
    "\n",
    "> Neural Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.tree import BaseDecisionTree\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptotree.polynomials import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from sklearn.tree import BaseDecisionTree\n",
    "from sklearn.base import is_classifier\n",
    "\n",
    "from typing import Callable\n",
    "from numpy.polynomial.chebyshev import Chebyshev\n",
    "from numpy.polynomial import Polynomial\n",
    "from cryptotree.activations import create_linear_node_comparator\n",
    "\n",
    "DEFAULT_POLYNOMIAL_DEGREE = 16\n",
    "DEFAULT_DILATATION_FACTOR = 16\n",
    "DEFAULT_BOUND = 1.0\n",
    "\n",
    "class NeuralTreeMaker:\n",
    "    \"\"\"Base class of Neural Decision Trees.\"\"\"\n",
    "    def __init__(self, \n",
    "                 activation: Callable, \n",
    "                 create_linear_leaf_matcher: Callable,\n",
    "                 create_regression_head: Callable,\n",
    "                 create_classifier_head: Callable,\n",
    "                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,\n",
    "                 use_polynomial : bool = False, \n",
    "                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND):\n",
    "        \n",
    "        activation_fn = lambda x: activation(x * dilatation_factor)\n",
    "        if use_polynomial:\n",
    "            domain = [-bound, bound]\n",
    "            activation_fn_numpy = lambda x: activation_fn(torch.tensor(x))\n",
    "            self.activation = Chebyshev.interpolate(activation_fn_numpy,deg=polynomial_degree,domain=domain)\n",
    "            self.coeffs = Polynomial.cast(self.activation).coef\n",
    "        else:\n",
    "            self.activation = activation_fn\n",
    "            self.coeffs = None\n",
    "            \n",
    "        self.create_linear_leaf_matcher = create_linear_leaf_matcher\n",
    "        self.create_regression_head = create_regression_head\n",
    "        self.create_classifier_head = create_classifier_head\n",
    "        \n",
    "    def make_tree(self, tree: BaseDecisionTree):\n",
    "        if is_classifier(tree):\n",
    "            create_head = self.create_classifier_head\n",
    "        else:\n",
    "            create_head = self.create_regression_head\n",
    "        neural_tree = NeuralDecisionTree(tree, self.activation, self.create_linear_leaf_matcher, create_head)\n",
    "        return neural_tree\n",
    "\n",
    "class NeuralDecisionTree(nn.Module):\n",
    "    \"\"\"Base class of Neural Decision Trees.\"\"\"\n",
    "    def __init__(self, tree: BaseDecisionTree,\n",
    "                 activation: Callable,\n",
    "                 create_linear_leaf_matcher: Callable,\n",
    "                 create_head: Callable):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.comparator = create_linear_node_comparator(tree)\n",
    "        self.matcher = create_linear_leaf_matcher(tree)\n",
    "        \n",
    "        self.head = create_head(tree)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        comparisons = self.comparator(x)\n",
    "        comparisons = self.activation(comparisons)\n",
    "        \n",
    "        matches = self.matcher(comparisons)\n",
    "        matches = self.activation(matches)\n",
    "        \n",
    "        output = self.head(matches)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from cryptotree.activations import sigmoid_linear_leaf_matcher, sigmoid_classification_head\n",
    "\n",
    "def raise_error_wrong_tree(*args,**kwargs):\n",
    "    raise Exception(\"Wrong supervised tree used\")\n",
    "    \n",
    "class SigmoidTreeMaker(NeuralTreeMaker):\n",
    "    def __init__(self, dilatation_factor : float = DEFAULT_DILATATION_FACTOR,\n",
    "                 use_polynomial : bool = False, \n",
    "                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND, eps=0.5):\n",
    "        \n",
    "        activation = torch.sigmoid\n",
    "        create_linear_leaf_matcher = partial(sigmoid_linear_leaf_matcher,eps=eps)\n",
    "        create_classifier_head = sigmoid_classification_head\n",
    "        create_regression_head = raise_error_wrong_tree\n",
    "        \n",
    "        super().__init__(activation, \n",
    "                 create_linear_leaf_matcher,\n",
    "                 create_regression_head,\n",
    "                 create_classifier_head,\n",
    "                 dilatation_factor,\n",
    "                 use_polynomial, \n",
    "                 polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from cryptotree.activations import tanh_linear_leaf_matcher, tanh_classification_head\n",
    "\n",
    "class TanhTreeMaker(NeuralTreeMaker):\n",
    "    def __init__(self, dilatation_factor : float = DEFAULT_DILATATION_FACTOR,\n",
    "                 use_polynomial : bool = False, \n",
    "                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND, eps=0.5):\n",
    "        \n",
    "        activation = torch.tanh\n",
    "        create_linear_leaf_matcher = partial(tanh_linear_leaf_matcher,eps=eps)\n",
    "        create_classifier_head = tanh_classification_head\n",
    "        create_regression_head = raise_error_wrong_tree\n",
    "        \n",
    "        super().__init__(activation, \n",
    "                 create_linear_leaf_matcher,\n",
    "                 create_regression_head,\n",
    "                 create_classifier_head,\n",
    "                 dilatation_factor,\n",
    "                 use_polynomial, \n",
    "                 polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_output_range(m, i, o, threshold=1):\n",
    "    rows_outside_range = ((torch.abs(o) > threshold).float().sum(dim=1) > 0).numpy()\n",
    "    idx_outside_range = np.arange(len(rows_outside_range))[rows_outside_range]\n",
    "    \n",
    "    assert len(idx_outside_range) == 0, f\"\"\"Out of range outputs for module {m}: \\n \n",
    "    {idx_outside_range} \\n\n",
    "    Rows with outside range : \\n\n",
    "    {o.numpy()[idx_outside_range]}\"\"\"\n",
    "\n",
    "def register_output_check(model, threshold=1):\n",
    "    for c in model.children():\n",
    "        if isinstance(c,nn.Linear):\n",
    "            hook = partial(check_output_range, threshold=threshold)\n",
    "            c.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 13 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 3] <= 0.2916666641831398 else to node 2.\n",
      "\tnode=1 leaf node.\n",
      "\tnode=2 test node: go to node 3 if X[:, 2] <= 0.6637930870056152 else to node 8.\n",
      "\t\tnode=3 test node: go to node 4 if X[:, 3] <= 0.6458333432674408 else to node 5.\n",
      "\t\t\tnode=4 leaf node.\n",
      "\t\t\tnode=5 test node: go to node 6 if X[:, 1] <= 0.4583333283662796 else to node 7.\n",
      "\t\t\t\tnode=6 leaf node.\n",
      "\t\t\t\tnode=7 leaf node.\n",
      "\t\tnode=8 test node: go to node 9 if X[:, 3] <= 0.6875 else to node 12.\n",
      "\t\t\tnode=9 test node: go to node 10 if X[:, 3] <= 0.6458333432674408 else to node 11.\n",
      "\t\t\t\tnode=10 leaf node.\n",
      "\t\t\t\tnode=11 leaf node.\n",
      "\t\t\tnode=12 leaf node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "        \n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 432x288 with 1 Axes>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f739cfac190>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cryptotree.polynomials import plot_graph_function_approximation\n",
    "\n",
    "dilatation_factor = 16\n",
    "polynomial_degree = 16\n",
    "\n",
    "plot_graph_function_approximation(torch.sigmoid,dilatation_factor=dilatation_factor,polynomial_degree=polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_tree_maker = SigmoidTreeMaker(use_polynomial=True,\n",
    "                                  dilatation_factor=dilatation_factor, polynomial_degree=polynomial_degree)\n",
    "tanh_tree_maker = TanhTreeMaker(use_polynomial=True,\n",
    "                                  dilatation_factor=dilatation_factor, polynomial_degree=polynomial_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pad_tensor(tensor, target, dim=0, value=0):\n",
    "    # If the tensor is already at the target size we return it\n",
    "    if tensor.shape[dim] >= target:\n",
    "        return tensor\n",
    "    else:\n",
    "        shape = list(tensor.shape)\n",
    "        shape[dim] = target - tensor.shape[dim]\n",
    "\n",
    "        padding = torch.ones(shape) * value\n",
    "        output = torch.cat([tensor,padding], dim=dim)\n",
    "        return output\n",
    "\n",
    "def pad_neural_tree(neural_tree, n_nodes_max, n_leaves_max):\n",
    "    w0, b0 = neural_tree.comparator.weight.data.clone(), neural_tree.comparator.bias.data.clone()\n",
    "    \n",
    "    # First we pad the output size of the comparator\n",
    "    neural_tree.comparator = nn.Linear(w0.shape[1], n_nodes_max)\n",
    "    neural_tree.comparator.weight.data = pad_tensor(w0, n_nodes_max, dim=0)\n",
    "    neural_tree.comparator.bias.data = pad_tensor(b0, n_nodes_max, dim=0)\n",
    "    \n",
    "    w1, b1 = neural_tree.matcher.weight.data.clone(), neural_tree.matcher.bias.data.clone()\n",
    "    # Then we pad the output and the input size of the matcher\n",
    "    neural_tree.matcher = nn.Linear(n_nodes_max, n_leaves_max)\n",
    "    neural_tree.matcher.weight.data = pad_tensor(pad_tensor(w1, n_nodes_max, dim=1), n_leaves_max, dim=0)\n",
    "    neural_tree.matcher.bias.data = pad_tensor(b1, n_leaves_max, dim=0)\n",
    "    \n",
    "    w2, b2 = neural_tree.head.weight.data.clone(), neural_tree.head.bias.data.clone()\n",
    "    neural_tree.head = nn.Linear(n_leaves_max, w2.shape[0])\n",
    "    neural_tree.head.weight.data = pad_tensor(w2, n_leaves_max, dim =1)\n",
    "    neural_tree.head.bias.data = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor([0.2454, 0.5324, 0.1666, 0.0828, 0.1119, 0.0260, 0.0389],\n",
      "       grad_fn=<AddBackward0>) tensor([0.2454, 0.5324, 0.1666, 0.0828, 0.1119, 0.0260, 0.0389, 0.5000, 0.5000,\n",
      "        0.5000, 0.5000], grad_fn=<AddBackward0>)\n",
      "tensor([ 0.0373,  0.2490, -0.1505], grad_fn=<AddBackward0>) tensor([-1.1161, -0.7808, -1.3460], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_train[0]).float()\n",
    "\n",
    "comparisons = model.activation(model.comparator(x))\n",
    "padded_comparisons = model.activation(padded_model.comparator(x))\n",
    "\n",
    "print(comparisons.sum() == padded_comparisons.sum())\n",
    "\n",
    "match = model.activation(model.matcher(comparisons))\n",
    "padded_match = model.activation(padded_model.matcher(padded_comparisons))\n",
    "\n",
    "print(match, padded_match)\n",
    "\n",
    "output = model.head(match)\n",
    "padded_output = padded_model.head(padded_match)\n",
    "\n",
    "print(output, padded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024],\n",
       "         [-0.8293, -0.0488, -0.8293, -0.8049, -0.8293, -0.8049, -0.8293],\n",
       "         [-1.0000, -1.0000, -0.9268, -1.0000, -0.9268, -1.0000, -0.1463]]),\n",
       " tensor([0.9024, 0.8293, 1.0000]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head.weight.data, model.head.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.8293, -0.0488, -0.8293, -0.8049, -0.8293, -0.8049, -0.8293,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000, -1.0000, -0.9268, -1.0000, -0.9268, -1.0000, -0.1463,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " tensor([-0.2510, -0.2005, -0.1955]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_model.head.weight.data, padded_model.head.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1161, -0.7808, -1.3460], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_model.head(padded_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=11, out_features=3, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9464)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes_max = 10\n",
    "n_leaves_max = 11\n",
    "\n",
    "x = torch.tensor(X_train).float()\n",
    "tree = estimator\n",
    "\n",
    "model = sigmoid_tree_maker.make_tree(tree)\n",
    "pad_neural_tree(model, n_nodes_max, n_leaves_max)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "pred = output.argmax(dim=1)\n",
    "y = estimator.predict(X_train)\n",
    "(pred == torch.tensor(y)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X_train).float()\n",
    "tree = estimator\n",
    "model = tanh_tree_maker.make_tree(tree)\n",
    "register_output_check(model, threshold=1)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "pred = output.argmax(dim=1)\n",
    "y = estimator.predict(X_train)\n",
    "(pred == torch.tensor(y)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NeuralRandomForest(nn.Module):\n",
    "    def __init__(self, trees: List[BaseDecisionTree],\n",
    "                 tree_maker: NeuralTreeMaker, \n",
    "                 weights: torch.Tensor = None, trainable_weights:bool = False,\n",
    "                 bias: torch.Tensor = None, trainable_bias:bool = False):\n",
    "        \n",
    "        super(NeuralRandomForest, self).__init__()\n",
    "        \n",
    "        self.n_trees = len(trees)\n",
    "        self.activation = tree_maker.activation\n",
    "        \n",
    "        # First we need to create the neural trees\n",
    "        neural_trees = []\n",
    "        n_nodes = []\n",
    "        n_leaves = []\n",
    "        for tree in trees:\n",
    "            neural_tree = tree_maker.make_tree(tree)\n",
    "            n_nodes.append(neural_tree.comparator.weight.data.shape[0])\n",
    "            n_leaves.append(neural_tree.matcher.weight.data.shape[0])\n",
    "            neural_trees.append(neural_tree)\n",
    "        \n",
    "        # Then we pad our neural trees according to the biggest tree in the forest\n",
    "        n_nodes_max = max(n_nodes)\n",
    "        n_leaves_max = max(n_leaves)\n",
    "        \n",
    "        for neural_tree in neural_trees:\n",
    "            pad_neural_tree(neural_tree, n_nodes_max, n_leaves_max)\n",
    "            \n",
    "        self.neural_trees = neural_trees\n",
    "        \n",
    "        # Then we create the parameters for the Neural Random Forest\n",
    "        comparators = [neural_tree.comparator.weight.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        comparator = torch.cat(comparators, dim=-1)\n",
    "        comparator = comparator.permute(1,0,2)\n",
    "        comparator = nn.Parameter(comparator)\n",
    "        self.register_parameter(\"comparator\", comparator)\n",
    "\n",
    "        comparator_bias = [neural_tree.comparator.bias.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        comparator_bias = torch.cat(comparator_bias, dim=-1)\n",
    "        comparator_bias = nn.Parameter(comparator_bias)\n",
    "        self.register_parameter(\"comparator_bias\", comparator_bias)\n",
    "\n",
    "        matchers = [neural_tree.matcher.weight.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        matcher = torch.cat(matchers, dim=-1)\n",
    "        matcher = nn.Parameter(matcher)\n",
    "        self.register_parameter(\"matcher\", matcher)\n",
    "\n",
    "        matcher_bias = [neural_tree.matcher.bias.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        matcher_bias = torch.cat(matcher_bias, dim=-1)\n",
    "        matcher_bias = nn.Parameter(matcher_bias)\n",
    "        self.register_parameter(\"matcher_bias\",matcher_bias)\n",
    "\n",
    "        heads = [neural_tree.head.weight.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        head = torch.cat(heads, dim=-1)\n",
    "        head = nn.Parameter(head)\n",
    "        self.register_parameter(\"head\", head)\n",
    "\n",
    "        head_bias = [neural_tree.head.bias.data.unsqueeze(-1) for neural_tree in neural_trees]\n",
    "        head_bias = torch.cat(head_bias, dim=-1)\n",
    "        head_bias = nn.Parameter(head_bias)\n",
    "        self.register_parameter(\"head_bias\", head_bias)\n",
    "        \n",
    "        if not torch.is_tensor(weights):\n",
    "            weights = torch.ones(self.n_trees) * (1. / self.n_trees)\n",
    "            \n",
    "        if trainable_weights:\n",
    "            weights = nn.Parameter(weights)\n",
    "            self.register_parameter(\"weights\", weights)\n",
    "        else:\n",
    "            self.register_buffer(\"weights\", weights)\n",
    "            \n",
    "        if not torch.is_tensor(bias):\n",
    "            c = neural_tree.head.weight.data.shape[0]\n",
    "            bias = torch.zeros(c)\n",
    "            \n",
    "        if trainable_bias:\n",
    "            bias = nn.Parameter(bias)\n",
    "            self.register_parameter(\"bias\",bias)\n",
    "        else:\n",
    "            self.register_buffer(\"bias\",bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        comparisons = torch.einsum(\"kj,jil->kil\",x,self.comparator) + self.comparator_bias.unsqueeze(0)\n",
    "        comparisons = self.activation(comparisons)\n",
    "        \n",
    "        matches = torch.einsum(\"kjl,ijl->kil\",comparisons, self.matcher) + self.matcher_bias\n",
    "        matches = self.activation(matches)\n",
    "        \n",
    "        outputs = torch.einsum(\"kjl,cjl->kcl\",matches,self.head) + self.head_bias\n",
    "        outputs = (outputs * self.weights.expand_as(outputs)).sum(dim=-1)\n",
    "        outputs = outputs + self.bias.expand_as(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2708, -0.2708, -0.3194,  ..., -0.2414, -0.6875, -0.2414],\n",
      "        [-0.6466, -0.6875, -0.2917,  ..., -0.6875, -0.4375, -0.6458],\n",
      "        [-0.6667, -0.6638, -0.6638,  ..., -0.6810, -0.2292, -0.5625],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[-0.1667, -0.1429, -0.2000,  ..., -0.2000, -0.1250, -0.1429],\n",
      "         [ 0.0000,  0.0000, -0.2000,  ...,  0.0000, -0.1250,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1250,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1667,  0.1429, -0.2000,  ...,  0.2000, -0.1250,  0.1429],\n",
      "         [-0.1667, -0.1429,  0.2000,  ..., -0.2000, -0.1250, -0.1429],\n",
      "         [-0.1667, -0.1429,  0.0000,  ..., -0.2000,  0.1250, -0.1429],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1667,  0.1429,  0.2000,  ...,  0.2000, -0.1250,  0.1429],\n",
      "         [-0.1667, -0.1429,  0.0000,  ..., -0.2000, -0.1250, -0.1429],\n",
      "         [ 0.1667, -0.1429, -0.2000,  ...,  0.2000,  0.1250,  0.1429],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0833,  0.0714,  0.1000,  ...,  0.1000,  0.0625,  0.0714],\n",
      "        [-0.0833, -0.0714, -0.1000,  ..., -0.1000, -0.0625, -0.0714],\n",
      "        [-0.2500, -0.2143, -0.1000,  ..., -0.3000, -0.1875, -0.2143],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[ 0.0000,  0.0000, -0.0476,  ...,  0.0000, -0.5400,  0.0000],\n",
      "         [-1.0000, -1.0000, -0.9524,  ..., -1.0000, -0.5800, -0.9286],\n",
      "         [-1.0000, -1.0000, -0.9048,  ..., -1.0000, -0.5800, -0.9286],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.0000, -0.8500, -0.7143,  ..., -0.8293, -0.6600, -0.7381],\n",
      "         [-0.1053, -0.3750, -0.6667,  ..., -0.0244, -0.1200, -0.1429],\n",
      "         [-1.0000, -0.5250, -0.7143,  ..., -0.8293, -0.6200, -0.7143],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9474, -0.9500, -1.0000,  ..., -0.9024, -1.0000, -1.0000],\n",
      "         [-0.9474, -0.9500, -1.0000,  ..., -0.9024, -1.0000, -1.0000],\n",
      "         [-0.9211, -0.9500, -1.0000,  ..., -0.8780, -0.9800, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1.0000, 1.0000, 0.9524, 0.7500, 1.0000, 1.0000, 0.7174, 1.0000, 0.9487,\n",
      "         0.8750, 0.8085, 0.9474, 0.7609, 0.6667, 0.8974, 0.8750, 0.7619, 0.7381,\n",
      "         1.0000, 0.6875, 0.9500, 1.0000, 1.0000, 1.0000, 0.8837, 0.8605, 0.8140,\n",
      "         0.9487, 0.7556, 1.0000, 0.5918, 1.0000, 0.8372, 0.9756, 0.8478, 0.9000,\n",
      "         0.5870, 0.9756, 1.0000, 0.6170, 0.6939, 0.8500, 1.0000, 0.8049, 0.7907,\n",
      "         1.0000, 0.8837, 1.0000, 0.8750, 1.0000, 1.0000, 0.8605, 0.8810, 0.7143,\n",
      "         0.8810, 0.7234, 1.0000, 0.6383, 1.0000, 0.8636, 0.9737, 0.8200, 0.9737,\n",
      "         1.0000, 0.5000, 0.5962, 1.0000, 0.8667, 1.0000, 0.8605, 0.7111, 0.9750,\n",
      "         0.8837, 0.6591, 1.0000, 1.0000, 0.8780, 0.7273, 1.0000, 0.6444, 0.8182,\n",
      "         0.9524, 1.0000, 0.7674, 0.7556, 0.7442, 0.6222, 0.9231, 0.8043, 0.7955,\n",
      "         1.0000, 0.9737, 0.7556, 1.0000, 0.9512, 0.6275, 0.8537, 1.0000, 0.5800,\n",
      "         0.9286],\n",
      "        [1.0000, 0.8500, 0.7143, 0.7955, 0.9512, 0.8293, 0.7174, 0.8571, 0.9231,\n",
      "         1.0000, 0.5745, 1.0000, 0.6739, 0.6667, 0.9744, 1.0000, 0.9048, 0.9286,\n",
      "         0.8750, 0.6458, 0.8500, 0.6667, 0.7619, 0.6905, 0.7209, 1.0000, 0.7907,\n",
      "         0.9231, 0.7333, 0.7907, 0.6939, 0.8780, 1.0000, 0.7561, 0.5870, 0.9000,\n",
      "         1.0000, 0.7561, 0.7674, 0.7660, 0.5918, 1.0000, 0.9286, 0.9268, 0.8140,\n",
      "         0.6279, 0.7209, 0.7727, 0.9250, 0.8750, 0.7021, 0.7442, 0.7857, 0.5714,\n",
      "         0.7857, 0.6596, 0.6739, 0.7447, 0.7674, 0.6818, 0.9737, 0.4200, 1.0000,\n",
      "         0.9500, 0.6538, 0.5577, 0.8500, 0.6222, 0.7561, 1.0000, 1.0000, 1.0000,\n",
      "         0.7209, 0.8864, 0.8000, 0.8049, 1.0000, 1.0000, 0.7778, 0.8444, 0.7273,\n",
      "         0.7143, 0.7857, 0.8372, 0.7333, 0.8605, 1.0000, 0.9487, 0.6304, 0.7500,\n",
      "         0.8537, 0.9737, 0.7333, 0.6444, 0.7805, 0.5686, 1.0000, 0.8293, 0.6600,\n",
      "         0.7381],\n",
      "        [0.9474, 0.9500, 1.0000, 1.0000, 0.7805, 0.9024, 1.0000, 0.8095, 1.0000,\n",
      "         0.9250, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9250, 1.0000, 1.0000,\n",
      "         0.9250, 1.0000, 1.0000, 1.0000, 0.9048, 0.9762, 1.0000, 0.7442, 1.0000,\n",
      "         1.0000, 1.0000, 0.8140, 1.0000, 0.8537, 0.7674, 1.0000, 1.0000, 1.0000,\n",
      "         0.8478, 1.0000, 0.8372, 1.0000, 1.0000, 0.9500, 0.7381, 1.0000, 1.0000,\n",
      "         0.9767, 1.0000, 0.7727, 1.0000, 0.9250, 0.6809, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 0.7609, 1.0000, 0.8372, 1.0000, 1.0000, 1.0000, 0.9737,\n",
      "         0.8500, 1.0000, 1.0000, 0.9500, 1.0000, 0.9756, 0.7442, 0.7778, 0.8250,\n",
      "         1.0000, 1.0000, 1.0000, 0.9268, 0.8537, 0.8182, 0.7111, 1.0000, 1.0000,\n",
      "         1.0000, 0.8810, 1.0000, 1.0000, 1.0000, 0.8667, 1.0000, 1.0000, 1.0000,\n",
      "         0.8780, 1.0000, 1.0000, 0.8444, 1.0000, 1.0000, 0.8780, 0.9024, 1.0000,\n",
      "         1.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in neural_rf.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=5, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = rf.estimators_\n",
    "make_tree = sigmoid_tree_maker.make_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rf = NeuralRandomForest(trees, make_tree)\n",
    "neural_trees = neural_rf.neural_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rf.register_parameter(\"comparator\",nn.Parameter(comparator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0034,  0.1942,  0.1382,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.9541,  0.0110,  0.0770,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0032,  0.8935,  0.7826,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        ...,\n",
       "        [ 0.8058, -0.0034,  0.1053,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0044,  0.9541,  1.0042,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.0110, -0.0023, -0.0042,  ...,  0.5000,  0.5000,  0.5000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons = torch.einsum(\"kj,jil->kil\",x,comparator) + comparator_bias.unsqueeze(0)\n",
    "comparisons = sigmoid_tree_maker.activation(comparisons)\n",
    "comparisons[:,:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0034,  0.1942,  0.1382,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.9541,  0.0110,  0.0770,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0032,  0.8935,  0.7826,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        ...,\n",
       "        [ 0.8058, -0.0034,  0.1053,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 1.0044,  0.9541,  1.0042,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.0110, -0.0023, -0.0042,  ...,  0.5000,  0.5000,  0.5000]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_tree = neural_trees[i]\n",
    "comparison = neural_tree.activation(neural_tree.comparator(x))\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2787,  0.4121,  0.1801,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2988,  0.5473,  0.3143,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2788, -0.0041, -0.0035,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        ...,\n",
       "        [ 0.3619,  0.4505,  0.2877,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2784,  0.0014,  0.0014,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.7155,  0.2871,  0.0232,  ...,  0.5000,  0.5000,  0.5000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = torch.einsum(\"kjl,ijl->kil\",comparisons, matcher) + matcher_bias\n",
    "matches = sigmoid_tree_maker.activation(matches)\n",
    "matches[:,:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2787,  0.4121,  0.1801,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2988,  0.5473,  0.3143,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2788, -0.0041, -0.0035,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        ...,\n",
       "        [ 0.3619,  0.4505,  0.2877,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.2784,  0.0014,  0.0014,  ...,  0.5000,  0.5000,  0.5000],\n",
       "        [ 0.7155,  0.2871,  0.0232,  ...,  0.5000,  0.5000,  0.5000]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = neural_tree.activation(neural_tree.matcher(comparison))\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rf = NeuralRandomForest(trees, sigmoid_tree_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1402,  0.1001, -0.2013],\n",
       "        [-0.2012,  0.1106, -0.3262],\n",
       "        [-0.1494, -0.1972,  0.2097],\n",
       "        [ 0.5272,  0.0168, -0.1169],\n",
       "        [-0.1579, -0.1808,  0.1320],\n",
       "        [ 0.4465,  0.0636, -0.1217],\n",
       "        [ 0.4899,  0.0305, -0.1225],\n",
       "        [-0.1345,  0.0597, -0.3621],\n",
       "        [-0.0809, -0.1126,  0.2007],\n",
       "        [-0.1945, -0.2536,  0.2034],\n",
       "        [-0.0963,  0.0012, -0.0192],\n",
       "        [-0.1466, -0.1899,  0.1887],\n",
       "        [-0.1759,  0.1145, -0.3342],\n",
       "        [-0.2200, -0.2863,  0.2222],\n",
       "        [-0.0732,  0.0448, -0.0332],\n",
       "        [-0.1817,  0.1039, -0.3493],\n",
       "        [-0.0499, -0.0155,  0.1055],\n",
       "        [-0.0843,  0.0046,  0.0230],\n",
       "        [-0.0596, -0.0056, -0.0798],\n",
       "        [-0.0809, -0.0754,  0.1282],\n",
       "        [-0.1394,  0.0328, -0.1328],\n",
       "        [-0.0975, -0.1216,  0.1788],\n",
       "        [-0.0930,  0.0592, -0.1023],\n",
       "        [ 0.5081,  0.0297, -0.1172],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [-0.1655,  0.1112, -0.2705],\n",
       "        [-0.1889,  0.1166, -0.3364],\n",
       "        [-0.1992,  0.0910, -0.3207],\n",
       "        [-0.1152,  0.0924, -0.1379],\n",
       "        [-0.1552, -0.2115,  0.1875],\n",
       "        [ 0.4982,  0.0299, -0.1202],\n",
       "        [ 0.5235,  0.0027, -0.1261],\n",
       "        [-0.0847, -0.1090,  0.1921],\n",
       "        [-0.1501,  0.1289, -0.2749],\n",
       "        [ 0.4759,  0.0449, -0.1213],\n",
       "        [ 0.4454,  0.0366, -0.1314],\n",
       "        [-0.0932,  0.0157, -0.0240],\n",
       "        [ 0.5124,  0.0187, -0.1219],\n",
       "        [-0.0204, -0.0197,  0.1497],\n",
       "        [-0.0779,  0.1048, -0.3172],\n",
       "        [ 0.5155,  0.0152, -0.1228],\n",
       "        [-0.1483,  0.0445, -0.1506],\n",
       "        [-0.2404, -0.3085,  0.1908],\n",
       "        [-0.1595,  0.1205, -0.2642],\n",
       "        [ 0.5204,  0.0223, -0.1162],\n",
       "        [-0.1864, -0.2483,  0.2262],\n",
       "        [-0.1941, -0.2525,  0.2070],\n",
       "        [-0.1781, -0.2361,  0.2318],\n",
       "        [-0.1519, -0.2000,  0.2226],\n",
       "        [ 0.5019,  0.0277, -0.1203],\n",
       "        [ 0.5094,  0.0240, -0.1203],\n",
       "        [-0.1029, -0.1191,  0.1443],\n",
       "        [-0.1786, -0.2354,  0.2024],\n",
       "        [ 0.4897,  0.0010, -0.1350],\n",
       "        [-0.1694, -0.2130,  0.1620],\n",
       "        [ 0.5187,  0.0281, -0.1137],\n",
       "        [-0.0884, -0.0976,  0.1094],\n",
       "        [-0.1464, -0.1902,  0.1915],\n",
       "        [ 0.5210,  0.0078, -0.1250],\n",
       "        [ 0.5200,  0.0216, -0.1171],\n",
       "        [-0.1531, -0.1699,  0.1117],\n",
       "        [ 0.5130,  0.0314, -0.1142],\n",
       "        [ 0.5163,  0.0086, -0.1261],\n",
       "        [ 0.4590,  0.0492, -0.1232],\n",
       "        [-0.2109,  0.0761, -0.3311],\n",
       "        [-0.2040, -0.2674,  0.2189],\n",
       "        [-0.2102, -0.2797,  0.2274],\n",
       "        [ 0.5177,  0.0269, -0.1151],\n",
       "        [ 0.5155,  0.0307, -0.1137],\n",
       "        [ 0.5135,  0.0320, -0.1138],\n",
       "        [-0.1230,  0.1118, -0.1940],\n",
       "        [-0.1660,  0.0738, -0.2015],\n",
       "        [ 0.5308,  0.0267, -0.1094],\n",
       "        [ 0.4653,  0.0653, -0.1131],\n",
       "        [-0.1894,  0.0898, -0.2560],\n",
       "        [ 0.4778,  0.0506, -0.1165],\n",
       "        [-0.2431, -0.3075,  0.2159],\n",
       "        [-0.0988,  0.0770, -0.3425],\n",
       "        [-0.2294, -0.2996,  0.1761],\n",
       "        [-0.1692,  0.0662, -0.2063],\n",
       "        [ 0.5065,  0.0237, -0.1194],\n",
       "        [-0.0336,  0.0124,  0.0787],\n",
       "        [ 0.4655,  0.0130, -0.1333],\n",
       "        [-0.2034, -0.2694,  0.2204],\n",
       "        [ 0.5048,  0.0289, -0.1180],\n",
       "        [ 0.5084,  0.0244, -0.1202],\n",
       "        [-0.1762, -0.2360,  0.2118],\n",
       "        [ 0.5222,  0.0286, -0.1124],\n",
       "        [-0.1863, -0.2403,  0.1862],\n",
       "        [-0.1775,  0.1164, -0.2873],\n",
       "        [-0.2018,  0.1165, -0.3342],\n",
       "        [-0.1311,  0.1344, -0.3297],\n",
       "        [-0.1554, -0.1922,  0.1692],\n",
       "        [-0.1517, -0.1814,  0.1628],\n",
       "        [-0.0982,  0.0239, -0.0291],\n",
       "        [-0.0430, -0.0081,  0.1053],\n",
       "        [ 0.5042,  0.0386, -0.1137],\n",
       "        [-0.1389,  0.1152, -0.3346],\n",
       "        [-0.1772, -0.2317,  0.2127],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [ 0.5123,  0.0322, -0.1145],\n",
       "        [-0.1568,  0.0677, -0.1822],\n",
       "        [-0.1633,  0.1192, -0.2843],\n",
       "        [-0.0350,  0.0059,  0.0635],\n",
       "        [-0.2055,  0.0569, -0.2577],\n",
       "        [ 0.4926,  0.0285, -0.1218],\n",
       "        [ 0.4953,  0.0377, -0.1167],\n",
       "        [ 0.5173,  0.0290, -0.1137],\n",
       "        [-0.0885, -0.1234,  0.2061],\n",
       "        [-0.1119,  0.1406, -0.2743],\n",
       "        [-0.2530, -0.3238,  0.1831],\n",
       "        [ 0.5202,  0.0152, -0.1209]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_rf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2094, -0.1543, -0.3201],\n",
       "        [-0.4081, -0.2345, -0.5845],\n",
       "        [-0.2032, -0.4010,  0.1009],\n",
       "        [ 0.6389,  0.0795, -0.0686],\n",
       "        [-0.1821, -0.3839,  0.1545],\n",
       "        [ 0.4033,  0.0597, -0.1725],\n",
       "        [ 0.6507,  0.0890, -0.0590],\n",
       "        [-0.2741, -0.1713, -0.5795],\n",
       "        [-0.0766, -0.2923,  0.0682],\n",
       "        [-0.2648, -0.4516,  0.0684],\n",
       "        [-0.1622, -0.2518, -0.2379],\n",
       "        [-0.1422, -0.3507,  0.0347],\n",
       "        [-0.3523, -0.2080, -0.5895],\n",
       "        [-0.3145, -0.4896,  0.0278],\n",
       "        [-0.0635, -0.1677, -0.0499],\n",
       "        [-0.3714, -0.2138, -0.6152],\n",
       "        [ 0.0281, -0.1779,  0.1180],\n",
       "        [-0.1954, -0.2939, -0.1929],\n",
       "        [-0.0440, -0.1530, -0.2276],\n",
       "        [-0.0129, -0.2310,  0.1271],\n",
       "        [-0.3489, -0.3283, -0.2959],\n",
       "        [-0.1016, -0.3155,  0.1315],\n",
       "        [-0.0580, -0.1276, -0.1269],\n",
       "        [ 0.6400,  0.0797, -0.0675],\n",
       "        [-0.0152, -0.2325, -0.0283],\n",
       "        [-0.4153, -0.2643, -0.5918],\n",
       "        [-0.4487, -0.2269, -0.6289],\n",
       "        [-0.4454, -0.2595, -0.6319],\n",
       "        [-0.2845, -0.2528, -0.3548],\n",
       "        [-0.1739, -0.3783,  0.0151],\n",
       "        [ 0.6462,  0.0852, -0.0625],\n",
       "        [ 0.6513,  0.0810, -0.0622],\n",
       "        [-0.0616, -0.2814,  0.1068],\n",
       "        [-0.3993, -0.2559, -0.5867],\n",
       "        [ 0.6525,  0.0895, -0.0573],\n",
       "        [ 0.6040,  0.1054, -0.0684],\n",
       "        [-0.2278, -0.2964, -0.1541],\n",
       "        [ 0.6402,  0.0802, -0.0674],\n",
       "        [ 0.0248, -0.1905, -0.0025],\n",
       "        [-0.4141, -0.2230, -0.6687],\n",
       "        [ 0.6150,  0.0798, -0.0780],\n",
       "        [-0.1856, -0.1901, -0.2193],\n",
       "        [-0.3071, -0.4851,  0.0370],\n",
       "        [-0.4036, -0.2550, -0.5690],\n",
       "        [ 0.6500,  0.0800, -0.0633],\n",
       "        [-0.2624, -0.4484,  0.0492],\n",
       "        [-0.2718, -0.4562,  0.0700],\n",
       "        [-0.2742, -0.4564,  0.0248],\n",
       "        [-0.2143, -0.4083,  0.0405],\n",
       "        [ 0.6413,  0.0808, -0.0664],\n",
       "        [ 0.6400,  0.0797, -0.0675],\n",
       "        [-0.0873, -0.3018,  0.1448],\n",
       "        [-0.2345, -0.4258,  0.0654],\n",
       "        [ 0.6151,  0.0800, -0.0780],\n",
       "        [-0.2254, -0.4166,  0.0118],\n",
       "        [ 0.6384,  0.0790, -0.0689],\n",
       "        [-0.0652, -0.2738, -0.0643],\n",
       "        [-0.1867, -0.3877,  0.1195],\n",
       "        [ 0.6398,  0.0803, -0.0678],\n",
       "        [ 0.6388,  0.0793, -0.0686],\n",
       "        [-0.2426, -0.4079, -0.1262],\n",
       "        [ 0.6153,  0.0801, -0.0778],\n",
       "        [ 0.6398,  0.0803, -0.0678],\n",
       "        [ 0.5870,  0.0904, -0.0825],\n",
       "        [-0.4454, -0.2595, -0.6319],\n",
       "        [-0.2966, -0.4755,  0.0438],\n",
       "        [-0.2983, -0.4767,  0.0123],\n",
       "        [ 0.6400,  0.0803, -0.0677],\n",
       "        [ 0.6390,  0.0792, -0.0684],\n",
       "        [ 0.6422,  0.0822, -0.0658],\n",
       "        [-0.3727, -0.2842, -0.5344],\n",
       "        [-0.3578, -0.2781, -0.3304],\n",
       "        [ 0.6385,  0.0792, -0.0689],\n",
       "        [ 0.5066,  0.0757, -0.1219],\n",
       "        [-0.4177, -0.2879, -0.4436],\n",
       "        [ 0.5852,  0.0899, -0.0841],\n",
       "        [-0.3282, -0.5004,  0.0150],\n",
       "        [-0.2777, -0.1685, -0.5833],\n",
       "        [-0.2591, -0.4470,  0.0208],\n",
       "        [-0.3654, -0.2847, -0.3278],\n",
       "        [ 0.6541,  0.0832, -0.0599],\n",
       "        [ 0.0466, -0.1509,  0.0462],\n",
       "        [ 0.6689,  0.1044, -0.0453],\n",
       "        [-0.2607, -0.4482,  0.0170],\n",
       "        [ 0.6159,  0.0803, -0.0772],\n",
       "        [ 0.6405,  0.0801, -0.0671],\n",
       "        [-0.2027, -0.4029,  0.0297],\n",
       "        [ 0.6388,  0.0793, -0.0686],\n",
       "        [-0.2557, -0.4422,  0.0130],\n",
       "        [-0.4130, -0.2472, -0.5792],\n",
       "        [-0.4148, -0.2283, -0.5918],\n",
       "        [-0.2113, -0.1201, -0.4977],\n",
       "        [-0.2100, -0.4058,  0.0767],\n",
       "        [-0.1955, -0.3951,  0.1327],\n",
       "        [-0.1805, -0.2551, -0.1830],\n",
       "        [-0.0271, -0.2116,  0.0924],\n",
       "        [ 0.6401,  0.0798, -0.0674],\n",
       "        [-0.2277, -0.1446, -0.5254],\n",
       "        [-0.2556, -0.4441,  0.0782],\n",
       "        [-0.0152, -0.2325, -0.0283],\n",
       "        [ 0.6405,  0.0805, -0.0671],\n",
       "        [-0.4108, -0.3402, -0.4016],\n",
       "        [-0.4247, -0.2564, -0.6021],\n",
       "        [ 0.0466, -0.1499,  0.0157],\n",
       "        [-0.3945, -0.2865, -0.4393],\n",
       "        [ 0.6551,  0.0929, -0.0557],\n",
       "        [ 0.5728,  0.0796, -0.0944],\n",
       "        [ 0.6504,  0.0801, -0.0629],\n",
       "        [-0.0808, -0.2960,  0.0211],\n",
       "        [-0.1574, -0.1312, -0.4326],\n",
       "        [-0.3220, -0.4950,  0.0113],\n",
       "        [ 0.6396,  0.0800, -0.0679]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.einsum(\"kjl,cjl->kcl\",matches,head) + head_bias\n",
    "outputs[:,:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2094, -0.1543, -0.3201],\n",
       "        [-0.4081, -0.2345, -0.5845],\n",
       "        [-0.2032, -0.4010,  0.1009],\n",
       "        [ 0.6389,  0.0795, -0.0686],\n",
       "        [-0.1821, -0.3839,  0.1545],\n",
       "        [ 0.4033,  0.0597, -0.1725],\n",
       "        [ 0.6507,  0.0890, -0.0590],\n",
       "        [-0.2741, -0.1713, -0.5795],\n",
       "        [-0.0766, -0.2923,  0.0682],\n",
       "        [-0.2648, -0.4516,  0.0684],\n",
       "        [-0.1622, -0.2518, -0.2379],\n",
       "        [-0.1422, -0.3507,  0.0347],\n",
       "        [-0.3523, -0.2080, -0.5895],\n",
       "        [-0.3145, -0.4896,  0.0278],\n",
       "        [-0.0635, -0.1677, -0.0499],\n",
       "        [-0.3714, -0.2138, -0.6152],\n",
       "        [ 0.0281, -0.1779,  0.1180],\n",
       "        [-0.1954, -0.2939, -0.1929],\n",
       "        [-0.0440, -0.1530, -0.2276],\n",
       "        [-0.0129, -0.2310,  0.1271],\n",
       "        [-0.3489, -0.3283, -0.2959],\n",
       "        [-0.1016, -0.3155,  0.1315],\n",
       "        [-0.0580, -0.1276, -0.1269],\n",
       "        [ 0.6400,  0.0797, -0.0675],\n",
       "        [-0.0152, -0.2325, -0.0283],\n",
       "        [-0.4153, -0.2643, -0.5918],\n",
       "        [-0.4487, -0.2269, -0.6289],\n",
       "        [-0.4454, -0.2595, -0.6319],\n",
       "        [-0.2845, -0.2528, -0.3548],\n",
       "        [-0.1739, -0.3783,  0.0151],\n",
       "        [ 0.6462,  0.0852, -0.0625],\n",
       "        [ 0.6513,  0.0810, -0.0622],\n",
       "        [-0.0616, -0.2814,  0.1068],\n",
       "        [-0.3993, -0.2559, -0.5867],\n",
       "        [ 0.6525,  0.0895, -0.0573],\n",
       "        [ 0.6040,  0.1054, -0.0684],\n",
       "        [-0.2278, -0.2964, -0.1541],\n",
       "        [ 0.6402,  0.0802, -0.0674],\n",
       "        [ 0.0248, -0.1905, -0.0025],\n",
       "        [-0.4141, -0.2230, -0.6687],\n",
       "        [ 0.6150,  0.0798, -0.0780],\n",
       "        [-0.1856, -0.1901, -0.2193],\n",
       "        [-0.3071, -0.4851,  0.0370],\n",
       "        [-0.4036, -0.2550, -0.5690],\n",
       "        [ 0.6500,  0.0800, -0.0633],\n",
       "        [-0.2624, -0.4484,  0.0492],\n",
       "        [-0.2718, -0.4562,  0.0700],\n",
       "        [-0.2742, -0.4564,  0.0248],\n",
       "        [-0.2143, -0.4083,  0.0405],\n",
       "        [ 0.6413,  0.0808, -0.0664],\n",
       "        [ 0.6400,  0.0797, -0.0675],\n",
       "        [-0.0873, -0.3018,  0.1448],\n",
       "        [-0.2345, -0.4258,  0.0654],\n",
       "        [ 0.6151,  0.0800, -0.0780],\n",
       "        [-0.2254, -0.4166,  0.0118],\n",
       "        [ 0.6384,  0.0790, -0.0689],\n",
       "        [-0.0652, -0.2738, -0.0643],\n",
       "        [-0.1867, -0.3877,  0.1195],\n",
       "        [ 0.6398,  0.0803, -0.0678],\n",
       "        [ 0.6388,  0.0793, -0.0686],\n",
       "        [-0.2426, -0.4079, -0.1262],\n",
       "        [ 0.6153,  0.0801, -0.0778],\n",
       "        [ 0.6398,  0.0803, -0.0678],\n",
       "        [ 0.5870,  0.0904, -0.0825],\n",
       "        [-0.4454, -0.2595, -0.6319],\n",
       "        [-0.2966, -0.4755,  0.0438],\n",
       "        [-0.2983, -0.4767,  0.0123],\n",
       "        [ 0.6400,  0.0803, -0.0677],\n",
       "        [ 0.6390,  0.0792, -0.0684],\n",
       "        [ 0.6422,  0.0822, -0.0658],\n",
       "        [-0.3727, -0.2842, -0.5344],\n",
       "        [-0.3578, -0.2781, -0.3304],\n",
       "        [ 0.6385,  0.0792, -0.0689],\n",
       "        [ 0.5066,  0.0757, -0.1219],\n",
       "        [-0.4177, -0.2879, -0.4436],\n",
       "        [ 0.5852,  0.0899, -0.0841],\n",
       "        [-0.3282, -0.5004,  0.0150],\n",
       "        [-0.2777, -0.1685, -0.5833],\n",
       "        [-0.2591, -0.4470,  0.0208],\n",
       "        [-0.3654, -0.2847, -0.3278],\n",
       "        [ 0.6541,  0.0832, -0.0599],\n",
       "        [ 0.0466, -0.1509,  0.0462],\n",
       "        [ 0.6689,  0.1044, -0.0453],\n",
       "        [-0.2607, -0.4482,  0.0170],\n",
       "        [ 0.6159,  0.0803, -0.0772],\n",
       "        [ 0.6405,  0.0801, -0.0671],\n",
       "        [-0.2027, -0.4029,  0.0297],\n",
       "        [ 0.6388,  0.0793, -0.0686],\n",
       "        [-0.2557, -0.4422,  0.0130],\n",
       "        [-0.4130, -0.2472, -0.5792],\n",
       "        [-0.4148, -0.2283, -0.5918],\n",
       "        [-0.2113, -0.1201, -0.4977],\n",
       "        [-0.2100, -0.4058,  0.0767],\n",
       "        [-0.1955, -0.3951,  0.1327],\n",
       "        [-0.1805, -0.2551, -0.1830],\n",
       "        [-0.0271, -0.2116,  0.0924],\n",
       "        [ 0.6401,  0.0798, -0.0674],\n",
       "        [-0.2277, -0.1446, -0.5254],\n",
       "        [-0.2556, -0.4441,  0.0782],\n",
       "        [-0.0152, -0.2325, -0.0283],\n",
       "        [ 0.6405,  0.0805, -0.0671],\n",
       "        [-0.4108, -0.3402, -0.4016],\n",
       "        [-0.4247, -0.2564, -0.6021],\n",
       "        [ 0.0466, -0.1499,  0.0157],\n",
       "        [-0.3945, -0.2865, -0.4393],\n",
       "        [ 0.6551,  0.0929, -0.0557],\n",
       "        [ 0.5728,  0.0796, -0.0944],\n",
       "        [ 0.6504,  0.0801, -0.0629],\n",
       "        [-0.0808, -0.2960,  0.0211],\n",
       "        [-0.1574, -0.1312, -0.4326],\n",
       "        [-0.3220, -0.4950,  0.0113],\n",
       "        [ 0.6396,  0.0800, -0.0679]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_tree.head(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = len(trees)\n",
    "\n",
    "weights = torch.ones(n_trees) * (1. / n_trees)\n",
    "bias = torch.zeros(neural_tree.head.weight.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = (outputs * weights).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = neural_rf(x).argmax(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642857142857143"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1402,  0.1001, -0.2013],\n",
       "        [-0.2012,  0.1106, -0.3262],\n",
       "        [-0.1494, -0.1972,  0.2097],\n",
       "        [ 0.5272,  0.0168, -0.1169],\n",
       "        [-0.1579, -0.1808,  0.1320],\n",
       "        [ 0.4465,  0.0636, -0.1217],\n",
       "        [ 0.4899,  0.0305, -0.1225],\n",
       "        [-0.1345,  0.0597, -0.3621],\n",
       "        [-0.0809, -0.1126,  0.2007],\n",
       "        [-0.1945, -0.2536,  0.2034],\n",
       "        [-0.0963,  0.0012, -0.0192],\n",
       "        [-0.1466, -0.1899,  0.1887],\n",
       "        [-0.1759,  0.1145, -0.3342],\n",
       "        [-0.2200, -0.2863,  0.2222],\n",
       "        [-0.0732,  0.0448, -0.0332],\n",
       "        [-0.1817,  0.1039, -0.3493],\n",
       "        [-0.0499, -0.0155,  0.1055],\n",
       "        [-0.0843,  0.0046,  0.0230],\n",
       "        [-0.0596, -0.0056, -0.0798],\n",
       "        [-0.0809, -0.0754,  0.1282],\n",
       "        [-0.1394,  0.0328, -0.1328],\n",
       "        [-0.0975, -0.1216,  0.1788],\n",
       "        [-0.0930,  0.0592, -0.1023],\n",
       "        [ 0.5081,  0.0297, -0.1172],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [-0.1655,  0.1112, -0.2705],\n",
       "        [-0.1889,  0.1166, -0.3364],\n",
       "        [-0.1992,  0.0910, -0.3207],\n",
       "        [-0.1152,  0.0924, -0.1379],\n",
       "        [-0.1552, -0.2115,  0.1875],\n",
       "        [ 0.4982,  0.0299, -0.1202],\n",
       "        [ 0.5235,  0.0027, -0.1261],\n",
       "        [-0.0847, -0.1090,  0.1921],\n",
       "        [-0.1501,  0.1289, -0.2749],\n",
       "        [ 0.4759,  0.0449, -0.1213],\n",
       "        [ 0.4454,  0.0366, -0.1314],\n",
       "        [-0.0932,  0.0157, -0.0240],\n",
       "        [ 0.5124,  0.0187, -0.1219],\n",
       "        [-0.0204, -0.0197,  0.1497],\n",
       "        [-0.0779,  0.1048, -0.3172],\n",
       "        [ 0.5155,  0.0152, -0.1228],\n",
       "        [-0.1483,  0.0445, -0.1506],\n",
       "        [-0.2404, -0.3085,  0.1908],\n",
       "        [-0.1595,  0.1205, -0.2642],\n",
       "        [ 0.5204,  0.0223, -0.1162],\n",
       "        [-0.1864, -0.2483,  0.2262],\n",
       "        [-0.1941, -0.2525,  0.2070],\n",
       "        [-0.1781, -0.2361,  0.2318],\n",
       "        [-0.1519, -0.2000,  0.2226],\n",
       "        [ 0.5019,  0.0277, -0.1203],\n",
       "        [ 0.5094,  0.0240, -0.1203],\n",
       "        [-0.1029, -0.1191,  0.1443],\n",
       "        [-0.1786, -0.2354,  0.2024],\n",
       "        [ 0.4897,  0.0010, -0.1350],\n",
       "        [-0.1694, -0.2130,  0.1620],\n",
       "        [ 0.5187,  0.0281, -0.1137],\n",
       "        [-0.0884, -0.0976,  0.1094],\n",
       "        [-0.1464, -0.1902,  0.1915],\n",
       "        [ 0.5210,  0.0078, -0.1250],\n",
       "        [ 0.5200,  0.0216, -0.1171],\n",
       "        [-0.1531, -0.1699,  0.1117],\n",
       "        [ 0.5130,  0.0314, -0.1142],\n",
       "        [ 0.5163,  0.0086, -0.1261],\n",
       "        [ 0.4590,  0.0492, -0.1232],\n",
       "        [-0.2109,  0.0761, -0.3311],\n",
       "        [-0.2040, -0.2674,  0.2189],\n",
       "        [-0.2102, -0.2797,  0.2274],\n",
       "        [ 0.5177,  0.0269, -0.1151],\n",
       "        [ 0.5155,  0.0307, -0.1137],\n",
       "        [ 0.5135,  0.0320, -0.1138],\n",
       "        [-0.1230,  0.1118, -0.1940],\n",
       "        [-0.1660,  0.0738, -0.2015],\n",
       "        [ 0.5308,  0.0267, -0.1094],\n",
       "        [ 0.4653,  0.0653, -0.1131],\n",
       "        [-0.1894,  0.0898, -0.2560],\n",
       "        [ 0.4778,  0.0506, -0.1165],\n",
       "        [-0.2431, -0.3075,  0.2159],\n",
       "        [-0.0988,  0.0770, -0.3425],\n",
       "        [-0.2294, -0.2996,  0.1761],\n",
       "        [-0.1692,  0.0662, -0.2063],\n",
       "        [ 0.5065,  0.0237, -0.1194],\n",
       "        [-0.0336,  0.0124,  0.0787],\n",
       "        [ 0.4655,  0.0130, -0.1333],\n",
       "        [-0.2034, -0.2694,  0.2204],\n",
       "        [ 0.5048,  0.0289, -0.1180],\n",
       "        [ 0.5084,  0.0244, -0.1202],\n",
       "        [-0.1762, -0.2360,  0.2118],\n",
       "        [ 0.5222,  0.0286, -0.1124],\n",
       "        [-0.1863, -0.2403,  0.1862],\n",
       "        [-0.1775,  0.1164, -0.2873],\n",
       "        [-0.2018,  0.1165, -0.3342],\n",
       "        [-0.1311,  0.1344, -0.3297],\n",
       "        [-0.1554, -0.1922,  0.1692],\n",
       "        [-0.1517, -0.1814,  0.1628],\n",
       "        [-0.0982,  0.0239, -0.0291],\n",
       "        [-0.0430, -0.0081,  0.1053],\n",
       "        [ 0.5042,  0.0386, -0.1137],\n",
       "        [-0.1389,  0.1152, -0.3346],\n",
       "        [-0.1772, -0.2317,  0.2127],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [ 0.5123,  0.0322, -0.1145],\n",
       "        [-0.1568,  0.0677, -0.1822],\n",
       "        [-0.1633,  0.1192, -0.2843],\n",
       "        [-0.0350,  0.0059,  0.0635],\n",
       "        [-0.2055,  0.0569, -0.2577],\n",
       "        [ 0.4926,  0.0285, -0.1218],\n",
       "        [ 0.4953,  0.0377, -0.1167],\n",
       "        [ 0.5173,  0.0290, -0.1137],\n",
       "        [-0.0885, -0.1234,  0.2061],\n",
       "        [-0.1119,  0.1406, -0.2743],\n",
       "        [-0.2530, -0.3238,  0.1831],\n",
       "        [ 0.5202,  0.0152, -0.1209]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs + bias.expand_as(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1402,  0.1001, -0.2013],\n",
       "        [-0.2012,  0.1106, -0.3262],\n",
       "        [-0.1494, -0.1972,  0.2097],\n",
       "        [ 0.5272,  0.0168, -0.1169],\n",
       "        [-0.1579, -0.1808,  0.1320],\n",
       "        [ 0.4465,  0.0636, -0.1217],\n",
       "        [ 0.4899,  0.0305, -0.1225],\n",
       "        [-0.1345,  0.0597, -0.3621],\n",
       "        [-0.0809, -0.1126,  0.2007],\n",
       "        [-0.1945, -0.2536,  0.2034],\n",
       "        [-0.0963,  0.0012, -0.0192],\n",
       "        [-0.1466, -0.1899,  0.1887],\n",
       "        [-0.1759,  0.1145, -0.3342],\n",
       "        [-0.2200, -0.2863,  0.2222],\n",
       "        [-0.0732,  0.0448, -0.0332],\n",
       "        [-0.1817,  0.1039, -0.3493],\n",
       "        [-0.0499, -0.0155,  0.1055],\n",
       "        [-0.0843,  0.0046,  0.0230],\n",
       "        [-0.0596, -0.0056, -0.0798],\n",
       "        [-0.0809, -0.0754,  0.1282],\n",
       "        [-0.1394,  0.0328, -0.1328],\n",
       "        [-0.0975, -0.1216,  0.1788],\n",
       "        [-0.0930,  0.0592, -0.1023],\n",
       "        [ 0.5081,  0.0297, -0.1172],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [-0.1655,  0.1112, -0.2705],\n",
       "        [-0.1889,  0.1166, -0.3364],\n",
       "        [-0.1992,  0.0910, -0.3207],\n",
       "        [-0.1152,  0.0924, -0.1379],\n",
       "        [-0.1552, -0.2115,  0.1875],\n",
       "        [ 0.4982,  0.0299, -0.1202],\n",
       "        [ 0.5235,  0.0027, -0.1261],\n",
       "        [-0.0847, -0.1090,  0.1921],\n",
       "        [-0.1501,  0.1289, -0.2749],\n",
       "        [ 0.4759,  0.0449, -0.1213],\n",
       "        [ 0.4454,  0.0366, -0.1314],\n",
       "        [-0.0932,  0.0157, -0.0240],\n",
       "        [ 0.5124,  0.0187, -0.1219],\n",
       "        [-0.0204, -0.0197,  0.1497],\n",
       "        [-0.0779,  0.1048, -0.3172],\n",
       "        [ 0.5155,  0.0152, -0.1228],\n",
       "        [-0.1483,  0.0445, -0.1506],\n",
       "        [-0.2404, -0.3085,  0.1908],\n",
       "        [-0.1595,  0.1205, -0.2642],\n",
       "        [ 0.5204,  0.0223, -0.1162],\n",
       "        [-0.1864, -0.2483,  0.2262],\n",
       "        [-0.1941, -0.2525,  0.2070],\n",
       "        [-0.1781, -0.2361,  0.2318],\n",
       "        [-0.1519, -0.2000,  0.2226],\n",
       "        [ 0.5019,  0.0277, -0.1203],\n",
       "        [ 0.5094,  0.0240, -0.1203],\n",
       "        [-0.1029, -0.1191,  0.1443],\n",
       "        [-0.1786, -0.2354,  0.2024],\n",
       "        [ 0.4897,  0.0010, -0.1350],\n",
       "        [-0.1694, -0.2130,  0.1620],\n",
       "        [ 0.5187,  0.0281, -0.1137],\n",
       "        [-0.0884, -0.0976,  0.1094],\n",
       "        [-0.1464, -0.1902,  0.1915],\n",
       "        [ 0.5210,  0.0078, -0.1250],\n",
       "        [ 0.5200,  0.0216, -0.1171],\n",
       "        [-0.1531, -0.1699,  0.1117],\n",
       "        [ 0.5130,  0.0314, -0.1142],\n",
       "        [ 0.5163,  0.0086, -0.1261],\n",
       "        [ 0.4590,  0.0492, -0.1232],\n",
       "        [-0.2109,  0.0761, -0.3311],\n",
       "        [-0.2040, -0.2674,  0.2189],\n",
       "        [-0.2102, -0.2797,  0.2274],\n",
       "        [ 0.5177,  0.0269, -0.1151],\n",
       "        [ 0.5155,  0.0307, -0.1137],\n",
       "        [ 0.5135,  0.0320, -0.1138],\n",
       "        [-0.1230,  0.1118, -0.1940],\n",
       "        [-0.1660,  0.0738, -0.2015],\n",
       "        [ 0.5308,  0.0267, -0.1094],\n",
       "        [ 0.4653,  0.0653, -0.1131],\n",
       "        [-0.1894,  0.0898, -0.2560],\n",
       "        [ 0.4778,  0.0506, -0.1165],\n",
       "        [-0.2431, -0.3075,  0.2159],\n",
       "        [-0.0988,  0.0770, -0.3425],\n",
       "        [-0.2294, -0.2996,  0.1761],\n",
       "        [-0.1692,  0.0662, -0.2063],\n",
       "        [ 0.5065,  0.0237, -0.1194],\n",
       "        [-0.0336,  0.0124,  0.0787],\n",
       "        [ 0.4655,  0.0130, -0.1333],\n",
       "        [-0.2034, -0.2694,  0.2204],\n",
       "        [ 0.5048,  0.0289, -0.1180],\n",
       "        [ 0.5084,  0.0244, -0.1202],\n",
       "        [-0.1762, -0.2360,  0.2118],\n",
       "        [ 0.5222,  0.0286, -0.1124],\n",
       "        [-0.1863, -0.2403,  0.1862],\n",
       "        [-0.1775,  0.1164, -0.2873],\n",
       "        [-0.2018,  0.1165, -0.3342],\n",
       "        [-0.1311,  0.1344, -0.3297],\n",
       "        [-0.1554, -0.1922,  0.1692],\n",
       "        [-0.1517, -0.1814,  0.1628],\n",
       "        [-0.0982,  0.0239, -0.0291],\n",
       "        [-0.0430, -0.0081,  0.1053],\n",
       "        [ 0.5042,  0.0386, -0.1137],\n",
       "        [-0.1389,  0.1152, -0.3346],\n",
       "        [-0.1772, -0.2317,  0.2127],\n",
       "        [-0.0513, -0.0619,  0.1489],\n",
       "        [ 0.5123,  0.0322, -0.1145],\n",
       "        [-0.1568,  0.0677, -0.1822],\n",
       "        [-0.1633,  0.1192, -0.2843],\n",
       "        [-0.0350,  0.0059,  0.0635],\n",
       "        [-0.2055,  0.0569, -0.2577],\n",
       "        [ 0.4926,  0.0285, -0.1218],\n",
       "        [ 0.4953,  0.0377, -0.1167],\n",
       "        [ 0.5173,  0.0290, -0.1137],\n",
       "        [-0.0885, -0.1234,  0.2061],\n",
       "        [-0.1119,  0.1406, -0.2743],\n",
       "        [-0.2530, -0.3238,  0.1831],\n",
       "        [ 0.5202,  0.0152, -0.1209]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs * weights.expand_as(outputs)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_einsum import contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 µs ± 7.43 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit contract(\"kj,jil->kil\",x,comparator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-14-b591051186ef>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-b591051186ef>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "neural_trees = []\n",
    "n_nodes = []\n",
    "n_leaves = []\n",
    "for tree in trees:\n",
    "    neural_tree = make_tree(tree)\n",
    "    n_nodes.append(neural_tree.comparator.weight.data.shape[0])\n",
    "    n_leaves.append(neural_tree.matcher.weight.data.shape[0])\n",
    "    neural_trees.append(neural_tree)\n",
    "\n",
    "n_nodes_max = max(n_nodes)\n",
    "n_leaves_max = max(n_leaves)\n",
    "\n",
    "for neural_tree in neural_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = sigmoid_tree_maker.make_tree(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]]) tensor([-0.2917, -0.6638, -0.6458, -0.4583, -0.6875, -0.6458])\n",
      "tensor([[-0.1667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667, -0.1667,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667, -0.1667,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667,  0.1667,  0.0000,  0.0000],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667, -0.1667],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667,  0.1667],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000,  0.1667,  0.0000]]) tensor([ 0.0833, -0.0833, -0.2500, -0.4167, -0.2500, -0.4167, -0.4167])\n",
      "tensor([[ 0.0000, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024],\n",
      "        [-0.8293, -0.0488, -0.8293, -0.8049, -0.8293, -0.8049, -0.8293],\n",
      "        [-1.0000, -1.0000, -0.9268, -1.0000, -0.9268, -1.0000, -0.1463]]) tensor([0.9024, 0.8293, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(tree.comparator.weight.data, tree.comparator.bias.data)\n",
    "print(tree.matcher.weight.data, tree.matcher.bias.data)\n",
    "print(tree.head.weight.data, tree.head.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) tensor([-0.2917, -0.6638, -0.6458, -0.4583, -0.6875, -0.6458,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000])\n",
      "tensor([[-0.1667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667, -0.1667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667, -0.1667,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667,  0.1667,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667, -0.1667,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667,  0.1667,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000,  0.1667,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]]) tensor([[-0.1667,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667, -0.1667,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667, -0.1667,  0.0000,  0.0000],\n",
      "        [ 0.1667, -0.1667,  0.1667,  0.1667,  0.0000,  0.0000],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667, -0.1667],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000, -0.1667,  0.1667],\n",
      "        [ 0.1667,  0.1667,  0.0000,  0.0000,  0.1667,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 0.0000, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024, -0.9024,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000],\n",
      "        [-0.8293, -0.0488, -0.8293, -0.8049, -0.8293, -0.8049, -0.8293,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000],\n",
      "        [-1.0000, -1.0000, -0.9268, -1.0000, -0.9268, -1.0000, -0.1463,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000]]) tensor([ 0.1029,  0.2476, -0.1051])\n"
     ]
    }
   ],
   "source": [
    "pad_neural_tree(tree, n_nodes_max=10, n_leaves_max=11)\n",
    "print(tree.comparator.weight.data, tree.comparator.bias.data)\n",
    "print(tree.matcher.weight.data, tree.matcher.bias.data)\n",
    "print(tree.head.weight.data, tree.head.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator2 = neural_trees[1].comparator.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2708, -0.6466, -0.6667, -0.4583, -0.6389, -0.7069,  0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tensor(neural_trees[0].comparator.bias.data, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_neural_tree(model, 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0833,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0833, -0.0833, -0.0833,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0833, -0.0833,  0.0833, -0.0833,  0.0000,  0.0000],\n",
       "        [ 0.0833, -0.0833,  0.0833,  0.0833,  0.0000,  0.0000],\n",
       "        [ 0.0833,  0.0833,  0.0000,  0.0000, -0.0833, -0.0833],\n",
       "        [ 0.0833,  0.0833,  0.0000,  0.0000, -0.0833,  0.0833],\n",
       "        [ 0.0833,  0.0833,  0.0000,  0.0000,  0.0833,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.matcher.weight.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
