# AUTOGENERATED! DO NOT EDIT! File to edit: nbs\00_core.ipynb (unless otherwise specified).

__all__ = ['shift_bit_eps', 'create_base_vectors', 'create_linear_system', 'BitComparison', 'create_test_cases_x',
           'create_test_cases_y', 'compute_leaves', 'create_linear_node_comparator', 'create_parent_of',
           'create_leaf_to_path', 'path_to_weight', 'sigmoid_path_to_linear', 'sigmoid_linear_leaf_matcher',
           'sigmoid_classification_head', 'DecisionTree', 'DEFAULT_POLYNOMIAL_DEGREE', 'DEFAULT_DILATATION_FACTOR',
           'DEFAULT_BOUND', 'raise_error_wrong_tree', 'SigmoidDecisionTree', 'SigmoidClassificationTree']

# Cell
import numpy as np
from typing import List
import torch.nn as nn
import torch
from sklearn.tree import BaseDecisionTree

def shift_bit_eps(bit: int, eps: float = 0.5):
    assert bit in [0,1], "Bit must be 0 or 1"
    return (2 * eps - 1) * bit + 1 - eps

def create_base_vectors(circuit: List[int], eps: float = 0.5):
    vectors = []
    n = len(circuit)

    for i in range(n):
        vector = list(circuit).copy()
        vector[i] = shift_bit_eps(vector[i], eps=eps)
        vectors.append(vector)

    return vectors

def create_linear_system(vectors: List[List[float]]):
    X = np.array(vectors)

    y = -X[:,-1]

    X[:,-1] = 1

    return X,y

class BitComparison(nn.Module):
    def __init__(self,target: List[int], eps : float = 0.5):
        super(BitComparison, self).__init__()

        vectors = create_base_vectors(target, eps=eps)
        X,y = create_linear_system(vectors)
        W = np.linalg.solve(X,y)
        w = W[:-1]
        w = np.concatenate([w,np.ones(1)])
        c = W[-1]

        if not target[-1]:
            w = -w
            c = -c

        n = len(target)
        self.n = n
        self.linear = nn.Linear(n,1)

        self.linear.weight.data = torch.tensor(w.reshape(1,-1)).float()
        self.linear.bias.data = torch.tensor(c).unsqueeze(0).float()

    def forward(self,x):
        return self.linear(x)

    def __repr__(self):
        output = ""
        for i in range(self.n):
            if i < self.n - 1:
                output += f"{self.linear.weight.data[0][i]}*x_{i} + "
            else:
                output += f"{self.linear.weight.data[0][i]}*y + "
        output += f"{self.linear.bias.data[0]} = 0"
        return output

# Cell
import itertools

def create_test_cases_x(n):
    products = [[0,1]] * n

    x = list(itertools.product(*products))
    x = np.array(x)

    return x

def create_test_cases_y(x,target):
    y = ((x == target).sum(axis=1) == n).astype(int)
    return y

# Cell
def compute_leaves(n_nodes, children_left, children_right):
    # The tree structure can be traversed to compute various properties such
    # as the depth of each node and whether or not it is a leaf.
    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)

    is_leaves = np.zeros(shape=n_nodes, dtype=bool)
    stack = [(0, -1)]  # seed is the root node id and its parent depth
    while len(stack) > 0:
        node_id, parent_depth = stack.pop()
        node_depth[node_id] = parent_depth + 1

        # If we have a test node
        if (children_left[node_id] != children_right[node_id]):
            stack.append((children_left[node_id], parent_depth + 1))
            stack.append((children_right[node_id], parent_depth + 1))
        else:
            is_leaves[node_id] = True

    return node_depth, is_leaves

# Cell
def create_linear_node_comparator(tree: BaseDecisionTree) -> nn.Linear:

    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right

    feature = tree.tree_.feature
    threshold = tree.tree_.threshold

    d = estimator.n_features_

    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)
    internal_nodes = [i for i,isLeaf in enumerate(is_leaves)if not isLeaf]

    W = []
    B = []

    for node in internal_nodes:
        w = np.zeros(d)
        w[feature[node]] = 1

        b = - threshold[node]
        W.append(w)
        B.append(b)

    W = np.stack(W)
    B = np.stack(B)

    linear = nn.Linear(W.shape[1],W.shape[0])
    linear.weight.data = torch.tensor(W).float()
    linear.bias.data = torch.tensor(B).float()

    return linear

# Cell
def create_parent_of(n_nodes, is_leaves, children_left, children_right):
    parentOf = {}

    for i in range(n_nodes):
        if not is_leaves[i]:
            parentOf[children_left[i]] = (i,0)
            parentOf[children_right[i]] = (i,1)
    return parentOf

# Cell
def create_leaf_to_path(n_nodes, is_leaves, children_left, children_right):
    parentOf = create_parent_of(n_nodes, is_leaves, children_left, children_right)
    leafToPath = []

    for i,isLeaf in enumerate(is_leaves):
        if isLeaf:
            node = i
            path = []

            parent = parentOf[node]

            while parent[0] != 0:
                path.append(parent)
                parent = parentOf[parent[0]]

            path.append(parent)
            leafToPath.append(path[::-1])

    return leafToPath

# Cell
def path_to_weight(path, nodes2idx):
    # This is the target of the Bitcomparison
    bits = [v for k,v in path]
    bit_comparison = BitComparison(bits)

    # Those are the indexes to be replaced by the corresponding weights
    idx = [nodes2idx[k] for k,v in path]

    K = len(nodes2idx)
    w = np.zeros(K)

    w[idx] = bit_comparison.linear.weight.data.numpy().reshape(-1)
    b = bit_comparison.linear.bias.data.numpy()

    return w,b

# Cell
def sigmoid_path_to_linear(leafToPath, nodes2idx):

    # For each leaf, we compute the linear layer to match it
    W = []
    B = []
    for path in leafToPath:
        w,b = path_to_weight(path,nodes2idx)
        W.append(w)
        B.append(b)

    W = np.stack(W)
    B = np.stack(B)

    # We divide the weights
    K = len(nodes2idx)

    linear = nn.Linear(W.shape[1],W.shape[0])
    linear.weight.data = torch.tensor(W).float() / K
    linear.bias.data = torch.tensor(B).view(-1).float() / K

    return linear

# Cell
def sigmoid_linear_leaf_matcher(tree: BaseDecisionTree) -> nn.Linear:

    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leafToPath = create_leaf_to_path(n_nodes,is_leaves,children_left,children_right)

    internal_nodes = [i for i,isLeaf in enumerate(is_leaves) if not isLeaf]
    nodes2idx = {node : i for i, node in enumerate(internal_nodes)}

    matcher = sigmoid_path_to_linear(leafToPath,nodes2idx)

    return matcher

# Cell
def sigmoid_classification_head(tree: BaseDecisionTree) -> nn.Linear:
    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leaves = [i for i,isLeaf in enumerate(is_leaves) if isLeaf]

    leaf_values = tree.tree_.value[leaves]
    leaf_values = torch.tensor(leaf_values).float()
    leaf_values = leaf_values.squeeze(1)

    head = nn.Linear(*leaf_values.shape,bias=False)
    head.weight.data = leaf_values.T / tree.tree_.value[0].max()

    return head

# Cell
from sklearn.tree import BaseDecisionTree
from sklearn.base import is_classifier

from typing import Callable
from numpy.polynomial.chebyshev import Chebyshev

DEFAULT_POLYNOMIAL_DEGREE = 25
DEFAULT_DILATATION_FACTOR = 100.0
DEFAULT_BOUND = 1.0

class DecisionTree(nn.Module):
    def __init__(self, tree: BaseDecisionTree,
                 activation: Callable,
                 create_linear_leaf_matcher: Callable,
                 create_regression_head: Callable,
                 create_classifier_head: Callable,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,
                 *args,**kwargs):
        super(DecisionTree, self).__init__()

        activation_fn = lambda x: activation(x * dilatation_factor)
        if use_polynomial:
            domain = [-bound, bound]
            activation_fn_numpy = lambda x: activation_fn(torch.tensor(x))
            self.activation = Chebyshev.interpolate(activation_fn_numpy,deg=polynomial_degree,domain=domain)
        else:
            self.activation = activation_fn

        self.comparator = create_linear_node_comparator(tree)
        self.matcher = create_linear_leaf_matcher(tree)

        if is_classifier(estimator):
            self.head = create_classifier_head(tree)
        else:
            self.head = create_regression_head(tree)

    def forward(self,x):
        comparisons = self.comparator(x)
        comparisons = self.activation(comparisons)

        matches = self.matcher(comparisons)
        matches = self.activation(matches)

        output = self.head(matches)

        return output

# Cell
def raise_error_wrong_tree(*args,**kwargs):
    raise Exception("Wrong supervised tree used")

class SigmoidDecisionTree(DecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 create_regression_head: Callable,
                 create_classifier_head: Callable,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND):
        activation = torch.sigmoid
        create_linear_leaf_matcher = sigmoid_linear_leaf_matcher

        super().__init__(tree,
                 activation=activation,
                 create_linear_leaf_matcher=create_linear_leaf_matcher,
                 create_regression_head=create_regression_head,
                 create_classifier_head=create_classifier_head,
                 dilatation_factor=dilatation_factor,
                 use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound)

class SigmoidClassificationTree(SigmoidDecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND):
        create_classifier_head = sigmoid_classification_head
        create_regression_head = raise_error_wrong_tree

        super().__init__(tree,
                         create_classifier_head=create_classifier_head,
                         create_regression_head=create_regression_head,
                         dilatation_factor=dilatation_factor,
                         use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound)