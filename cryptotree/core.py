# AUTOGENERATED! DO NOT EDIT! File to edit: nbs\00_core.ipynb (unless otherwise specified).

__all__ = ['shift_bit_eps', 'create_base_vectors', 'create_linear_system', 'BitComparison', 'compute_leaves',
           'create_linear_node_comparator', 'create_parent_of', 'create_leaf_to_path', 'tanh_path_to_weight',
           'tanh_path_to_linear', 'tanh_linear_leaf_matcher', 'sigmoid_path_to_weight', 'sigmoid_path_to_linear',
           'sigmoid_linear_leaf_matcher', 'sigmoid_classification_head', 'DecisionTree', 'DEFAULT_POLYNOMIAL_DEGREE',
           'DEFAULT_DILATATION_FACTOR', 'DEFAULT_BOUND', 'raise_error_wrong_tree', 'SigmoidDecisionTree',
           'SigmoidClassificationTree', 'tanh_classification_head', 'TanhDecisionTree', 'TanhClassificationTree',
           'check_output_range', 'register_output_check']

# Cell
import numpy as np
from typing import List
import torch.nn as nn
import torch
from sklearn.tree import BaseDecisionTree
from functools import partial

def shift_bit_eps(bit: int, eps: float = 0.5):
    assert bit in [0,1], "Bit must be 0 or 1"
    return (2 * eps - 1) * bit + 1 - eps

def create_base_vectors(circuit: List[int], eps: float = 0.5):
    vectors = []
    n = len(circuit)

    for i in range(n):
        vector = list(circuit).copy()
        vector[i] = shift_bit_eps(vector[i], eps=eps)
        vectors.append(vector)

    return vectors

def create_linear_system(vectors: List[List[float]]):
    X = np.array(vectors)

    y = -X[:,-1]

    X[:,-1] = 1

    return X,y

class BitComparison(nn.Module):
    def __init__(self,target: List[int], eps : float = 0.5):
        super(BitComparison, self).__init__()

        vectors = create_base_vectors(target, eps=eps)
        X,y = create_linear_system(vectors)
        W = np.linalg.solve(X,y)
        w = W[:-1]
        w = np.concatenate([w,np.ones(1)])
        c = W[-1]

        if not target[-1]:
            w = -w
            c = -c

        n = len(target)
        self.n = n
        self.linear = nn.Linear(n,1)

        self.linear.weight.data = torch.tensor(w.reshape(1,-1)).float()
        self.linear.bias.data = torch.tensor(c).unsqueeze(0).float()

    def forward(self,x):
        return self.linear(x)

    def __repr__(self):
        output = ""
        for i in range(self.n):
            if i < self.n - 1:
                output += f"{self.linear.weight.data[0][i]}*x_{i} + "
            else:
                output += f"{self.linear.weight.data[0][i]}*y + "
        output += f"{self.linear.bias.data[0]} = 0"
        return output

# Cell
def compute_leaves(n_nodes, children_left, children_right):
    # The tree structure can be traversed to compute various properties such
    # as the depth of each node and whether or not it is a leaf.
    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)

    is_leaves = np.zeros(shape=n_nodes, dtype=bool)
    stack = [(0, -1)]  # seed is the root node id and its parent depth
    while len(stack) > 0:
        node_id, parent_depth = stack.pop()
        node_depth[node_id] = parent_depth + 1

        # If we have a test node
        if (children_left[node_id] != children_right[node_id]):
            stack.append((children_left[node_id], parent_depth + 1))
            stack.append((children_right[node_id], parent_depth + 1))
        else:
            is_leaves[node_id] = True

    return node_depth, is_leaves

# Cell
def create_linear_node_comparator(tree: BaseDecisionTree) -> nn.Linear:

    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right

    feature = tree.tree_.feature
    threshold = tree.tree_.threshold

    d = estimator.n_features_

    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)
    internal_nodes = [i for i,isLeaf in enumerate(is_leaves)if not isLeaf]

    W = []
    B = []

    for node in internal_nodes:
        w = np.zeros(d)
        w[feature[node]] = 1

        b = - threshold[node]
        W.append(w)
        B.append(b)

    W = np.stack(W)
    B = np.stack(B)

    linear = nn.Linear(W.shape[1],W.shape[0])
    linear.weight.data = torch.tensor(W).float()
    linear.bias.data = torch.tensor(B).float()

    return linear

# Cell
def create_parent_of(n_nodes, is_leaves, children_left, children_right,left_value=0,right_value=1):
    parentOf = {}

    for i in range(n_nodes):
        if not is_leaves[i]:
            parentOf[children_left[i]] = (i,left_value)
            parentOf[children_right[i]] = (i,right_value)
    return parentOf

# Cell
def create_leaf_to_path(n_nodes, is_leaves, children_left, children_right,left_value=0,right_value=1):
    parentOf = create_parent_of(n_nodes, is_leaves, children_left, children_right,left_value,right_value)
    leafToPath = []

    for i,isLeaf in enumerate(is_leaves):
        if isLeaf:
            node = i
            path = []

            parent = parentOf[node]

            while parent[0] != 0:
                path.append(parent)
                parent = parentOf[parent[0]]

            path.append(parent)
            leafToPath.append(path[::-1])

    return leafToPath

# Cell
def tanh_path_to_weight(path, nodes2idx, node_depth, leaf, eps=0.5):
    bits = [v for k,v in path]
    assert (set(bits) == set([-1,1])) or (set(bits) == set([-1])) or (set(bits) == set([1])), f"Bits outside of -1 and 1 : {bits}"

    idx = [nodes2idx[k] for k,v in path]

    K = len(nodes2idx)
    w = np.zeros(K)

    w[idx] = bits
    b = -node_depth[leaf] + eps

    return w,b

# Cell
def tanh_path_to_linear(leafToPath, nodes2idx, idx2leaves, node_depth, eps=0.5):

    # For each leaf, we compute the linear layer to match it
    W = []
    B = []
    for leaf_id, path in enumerate(leafToPath):
        leaf = idx2leaves[leaf_id]
        w,b = tanh_path_to_weight(path, nodes2idx, node_depth, leaf, eps)
        W.append(w)
        B.append(b)

    W = np.stack(W)
    B = np.stack(B)

    # We divide the weights
    K = len(nodes2idx)

    linear = nn.Linear(W.shape[1],W.shape[0])
    linear.weight.data = torch.tensor(W).float() / (2 * K)
    linear.bias.data = torch.tensor(B).view(-1).float() / (2 * K)

    return linear

# Cell
def tanh_linear_leaf_matcher(tree: BaseDecisionTree, eps=0.5) -> nn.Linear:

    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leafToPath = create_leaf_to_path(n_nodes,is_leaves,children_left,children_right,left_value=-1)

    internal_nodes = [i for i,isLeaf in enumerate(is_leaves) if not isLeaf]
    leaves = [i for i,isLeaf in enumerate(is_leaves) if isLeaf]

    nodes2idx = {node : i for i, node in enumerate(internal_nodes)}
    leaves2idx = { leaf : i for i,leaf in enumerate(leaves)}

    matcher = tanh_path_to_linear(leafToPath, nodes2idx, idx2leaves, node_depth, eps)

    return matcher

# Cell
def sigmoid_path_to_weight(path, nodes2idx, eps=0.5):
    # This is the target of the Bitcomparison
    bits = [v for k,v in path]
    bit_comparison = BitComparison(bits, eps=eps)

    # Those are the indexes to be replaced by the corresponding weights
    idx = [nodes2idx[k] for k,v in path]

    K = len(nodes2idx)
    w = np.zeros(K)

    w[idx] = bit_comparison.linear.weight.data.numpy().reshape(-1)
    b = bit_comparison.linear.bias.data.numpy()

    return w,b

# Cell
def sigmoid_path_to_linear(leafToPath, nodes2idx, eps=0.5):

    # For each leaf, we compute the linear layer to match it
    W = []
    B = []
    for path in leafToPath:
        w,b = sigmoid_path_to_weight(path,nodes2idx, eps=eps)
        W.append(w)
        B.append(b)

    W = np.stack(W)
    B = np.stack(B)

    # We divide the weights
    K = len(nodes2idx)

    linear = nn.Linear(W.shape[1],W.shape[0])
    linear.weight.data = torch.tensor(W).float() / K
    linear.bias.data = torch.tensor(B).view(-1).float() / K

    return linear

# Cell
def sigmoid_linear_leaf_matcher(tree: BaseDecisionTree, eps=0.5) -> nn.Linear:

    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leafToPath = create_leaf_to_path(n_nodes,is_leaves,children_left,children_right)

    internal_nodes = [i for i,isLeaf in enumerate(is_leaves) if not isLeaf]
    nodes2idx = {node : i for i, node in enumerate(internal_nodes)}

    matcher = sigmoid_path_to_linear(leafToPath,nodes2idx, eps=0.5)

    return matcher

# Cell
def sigmoid_classification_head(tree: BaseDecisionTree) -> nn.Linear:
    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leaves = [i for i,isLeaf in enumerate(is_leaves) if isLeaf]

    leaf_values = tree.tree_.value[leaves]
    leaf_values = torch.tensor(leaf_values).float()
    leaf_values = leaf_values.squeeze(1)

    head = nn.Linear(*leaf_values.shape,bias=False)
    head.weight.data = leaf_values.T / tree.tree_.value[0].max()

    return head

# Cell
from sklearn.tree import BaseDecisionTree
from sklearn.base import is_classifier

from typing import Callable
from numpy.polynomial.chebyshev import Chebyshev

DEFAULT_POLYNOMIAL_DEGREE = 25
DEFAULT_DILATATION_FACTOR = 100.0
DEFAULT_BOUND = 1.0

class DecisionTree(nn.Module):
    def __init__(self, tree: BaseDecisionTree,
                 activation: Callable,
                 create_linear_leaf_matcher: Callable,
                 create_regression_head: Callable,
                 create_classifier_head: Callable,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,
                 *args,**kwargs):
        super(DecisionTree, self).__init__()

        activation_fn = lambda x: activation(x * dilatation_factor)
        if use_polynomial:
            domain = [-bound, bound]
            activation_fn_numpy = lambda x: activation_fn(torch.tensor(x))
            self.activation = Chebyshev.interpolate(activation_fn_numpy,deg=polynomial_degree,domain=domain)
        else:
            self.activation = activation_fn

        self.comparator = create_linear_node_comparator(tree)
        self.matcher = create_linear_leaf_matcher(tree)

        if is_classifier(estimator):
            self.head = create_classifier_head(tree)
        else:
            self.head = create_regression_head(tree)

    def forward(self,x):
        comparisons = self.comparator(x)
        comparisons = self.activation(comparisons)

        matches = self.matcher(comparisons)
        matches = self.activation(matches)

        output = self.head(matches)

        return output

# Cell
def raise_error_wrong_tree(*args,**kwargs):
    raise Exception("Wrong supervised tree used")

class SigmoidDecisionTree(DecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 create_regression_head: Callable,
                 create_classifier_head: Callable,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,eps=0.5):

        activation = torch.sigmoid
        create_linear_leaf_matcher = partial(sigmoid_linear_leaf_matcher,eps=eps)

        super().__init__(tree,
                 activation=activation,
                 create_linear_leaf_matcher=create_linear_leaf_matcher,
                 create_regression_head=create_regression_head,
                 create_classifier_head=create_classifier_head,
                 dilatation_factor=dilatation_factor,
                 use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound)

class SigmoidClassificationTree(SigmoidDecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,eps=0.5):
        create_classifier_head = sigmoid_classification_head
        create_regression_head = raise_error_wrong_tree

        super().__init__(tree,
                         create_classifier_head=create_classifier_head,
                         create_regression_head=create_regression_head,
                         dilatation_factor=dilatation_factor,
                         use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound,eps=eps)

# Cell
def tanh_classification_head(tree: BaseDecisionTree) -> nn.Linear:
    n_nodes = tree.tree_.node_count
    children_left = tree.tree_.children_left
    children_right = tree.tree_.children_right
    node_depth, is_leaves = compute_leaves(n_nodes, children_left, children_right)

    leaves = [i for i,isLeaf in enumerate(is_leaves) if isLeaf]

    leaf_values = tree.tree_.value[leaves]
    leaf_values = torch.tensor(leaf_values).float()

    leaf_values = leaf_values.squeeze(1) / tree.tree_.value[0].max()

    # We divide by 2 because we have -1 and 1 bits
    bias = leaf_values.sum(dim=0) / 2
    leaf_values = leaf_values / 2

    head = nn.Linear(*leaf_values.shape)
    head.weight.data = leaf_values.T
    head.bias.data = bias

    return head

# Cell
class TanhDecisionTree(DecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 create_regression_head: Callable,
                 create_classifier_head: Callable,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,eps=0.5):
        activation = torch.tanh
        create_linear_leaf_matcher = partial(tanh_linear_leaf_matcher,eps=eps)

        super().__init__(tree,
                 activation=activation,
                 create_linear_leaf_matcher=create_linear_leaf_matcher,
                 create_regression_head=create_regression_head,
                 create_classifier_head=create_classifier_head,
                 dilatation_factor=dilatation_factor,
                 use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound)

class TanhClassificationTree(TanhDecisionTree):
    def __init__(self, tree: BaseDecisionTree,
                 dilatation_factor : float = DEFAULT_DILATATION_FACTOR,
                 use_polynomial : bool = False,
                 polynomial_degree : int = DEFAULT_POLYNOMIAL_DEGREE, bound: float = DEFAULT_BOUND,eps=0.5):
        create_classifier_head = tanh_classification_head
        create_regression_head = raise_error_wrong_tree

        super().__init__(tree,
                         create_classifier_head=create_classifier_head,
                         create_regression_head=create_regression_head,
                         dilatation_factor=dilatation_factor,
                         use_polynomial=use_polynomial, polynomial_degree=polynomial_degree, bound=bound,eps=eps)

# Cell
def check_output_range(m, i, o, threshold=1):
    rows_outside_range = ((torch.abs(o) > threshold).float().sum(dim=1) > 0).numpy()
    idx_outside_range = np.arange(len(rows_outside_range))[rows_outside_range]

    assert len(idx_outside_range) == 0, f"""Out of range outputs for module {m}: \n
    {idx_outside_range} \n
    Rows with outside range : \n
    {o.numpy()[idx_outside_range]}"""

def register_output_check(model, threshold=1):
    for c in model.children():
        if isinstance(c,nn.Linear):
            hook = partial(check_output_range, threshold=threshold)
            c.register_forward_hook(hook)