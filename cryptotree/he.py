# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_homomorphic_tree.ipynb (unless otherwise specified).

__all__ = ['vrep', 'apply_comparator', 'extract_diagonals', 'matrix_multiply_diagonals', 'add_bias_ptx', 'LinearCtx',
           'pad_along_axis']

# Cell
def vrep(x, n):
    k = n // len(x)
    rest = n % len(x)
    output = x * k + x[:rest]
    return output

# Cell
import torch.nn as nn

def apply_comparator(ctx: Ciphertext, comparator: nn.Linear,
                     encoder: CKKSEncoder, evaluator: Evaluator) -> Ciphertext:
    n = encoder.slot_count()

    output = Ciphertext()
    row_ptx = Plaintext()

    # We repeat the bias vector
    bias_ptx = Plaintext()
    bias = list(comparator.bias.data.numpy())
    bias = repeat_vector(bias,n)
    bias = DoubleVector(bias)
    encoder.encode(bias, scale, bias_ptx)

    matrix = comparator.weight.data.numpy()

    for i in range(matrix.shape[0]):
        row = matrix[i]
        source = row.argmax()
        rotation = source - i

        # We encode the current row
        row = list(row)
        row = repeat_vector(row, n)
        row = DoubleVector(row)
        encoder.encode(row,scale, row_ptx)

        temp = Ciphertext()
        evaluator.multiply_plain(ctx, row_ptx, temp)
        evaluator.rescale_to_next_inplace(temp)

        # If we must rotate
        if np.abs(rotation) > 0:
            evaluator.rotate_vector_inplace(temp, rotation, galois_keys)

        if i == 0:
            output = temp
        else:
            evaluator.add_inplace(output,temp)

    evaluator.mod_switch_to_inplace(bias_ptx, output.parms_id())
    output.scale(scale)
    evaluator.add_plain_inplace(output, bias_ptx)

    return output

# Cell
def extract_diagonals(matrix: np.ndarray, encoder: CKKSEncoder) -> List[Plaintext]:
    """Extracts the diagonals of the matrix"""
    assert matrix.shape[0] == matrix.shape[1], "Non square matrix"
    dim = matrix.shape[0]

    diagonals = []

    for i in range(dim):
        diagonal = []
        for j in range(dim):
            diagonal.append(matrix[j][(j+i) % dim])
        diagonal_ptx = Plaintext()
        encoder.encode(DoubleVector(diagonal), scale, diagonal_ptx)
        diagonals.append(diagonal_ptx)
    return diagonals

# Cell
def matrix_multiply_diagonals(diagonals: List[Plaintext], ctx: Ciphertext,
                              evaluator: Evaluator, galois_keys: GaloisKeys):
    output = Ciphertext()

    for i in range(len(diagonals)):

        temp = Ciphertext()
        diagonal = diagonals[i]

        evaluator.rotate_vector(ctx, i, galois_keys, temp)

        evaluator.mod_switch_to_inplace(diagonal, temp.parms_id())
        evaluator.multiply_plain_inplace(temp, diagonal)
        evaluator.rescale_to_next_inplace(temp)

        if i == 0:
            output = temp
        else:
            evaluator.add_inplace(output, temp)

    return output

def add_bias_ptx(bias: np.ndarray, ctx: Ciphertext, evaluator: Evaluator):
    bias = DoubleVector(list(bias))
    bias_ptx = Plaintext()
    encoder.encode(bias, scale, bias_ptx)

    evaluator.mod_switch_to_inplace(bias_ptx, ctx.parms_id())
    ctx.scale(scale)

    output = Ciphertext()
    evaluator.add_plain(ctx, bias_ptx, output)
    return output

class LinearCtx():
    def __init__(self, matrix, bias):
        self.diagonals = extract_diagonals(matrix)
        self.bias = bias

    def __call__(self, ctx):
        pass

# Cell
def pad_along_axis(array: np.ndarray, target_length, axis=0):

    pad_size = target_length - array.shape[axis]
    axis_nb = len(array.shape)

    if pad_size <= 0:
        return array

    npad = [(0, 0) for x in range(axis_nb)]
    npad[axis] = (0, pad_size)

    b = np.pad(array, pad_width=npad, mode='constant', constant_values=0)

    return b